\section{Space Imaging Simulator for Proximity Operations} \label{sec:sispo}
\gls{sispo} is a software package that integrates trajectory simulation with physics-based rendering, image compression and \gls{3d} reconstruction. These functionalities are split into three sub-packages. The two main sub-packages are the \textit{sim} and the \textit{reconstruction} package. The third sub-package provides several image compression algorithms to test effects on reconstruction.

The software package is hosted on GitHub using a git version control system. Furthermore, the GitHub project management tools are used, including automated KanBan based projects, issue tracking and pull requests.

The most important python dependencies of \gls{sispo} are:
\begin{itemize}
    \item astropy: Astronomy package developed by~\cite{robitaille2013astropy} and~\cite{price2018astropy}
    \item Blender: \gls{3d} creation suite~\cite{blender}
    \item \textit{NumPy}: Scientific computing for python~\cite{oliphant2006guideNumPy}
    \item OpenCV: Computer vision library used for image processing~\cite{opencv_library}
    \item OpenEXR: \gls{hdr} image reading and writing~\cite{openexr}
    \item Orekit: Space dynamics library~\cite{orekit}
\end{itemize}

It was attempted to reduce dependencies to other libraries as much as possible. Originally, both \textit{scikit-image} and \textit{opencv} were used. After a small benchmark between the two libraries, it was evident that \textit{opencv} performs three to seven times faster compared to \textit{scikit-image} (cf. \ref{sec:cvskimage}. Hence \textit{scikit-image} was completely replaced with equivalent \textit{opencv} functions. Additionally, it was attempted to use the \textit{opencv} package to replace the \textit{OpenEXR} dependency since it is not easy to install. However, this is currently not possible as the \textit{OpenEXR} implementation of \textit{opencv} does not provide an alpha channel and is generally less flexible.

To ease development, numerous parameters are silently assigned default values if not provided e.g. for an instrument. Therefore, all parameters should be explicitly set before running a simulation.

\gls{sispo} was developed in an attempt to logically separate the distinct steps involved of the entire processing pipeline. Such modularity eases further development as well as reduce unnecessary library imports to reduce memory consumption.

\subsection{User Interface}
The \gls{sispo} package can be installed using pip and the GitHub project. If done, it will be installed into the site-packages folder of the used python environment. Additionally, an executable is installed, providing a \gls{cli} as user interface. The most recent possible input arguments are documented in the repository or by using the --help input. The most important arguments for the \gls{cli} are seen in Table~\ref{tab:cli_args}.

\begin{table}[htb]
    \centering
    \caption{Input arguments for sispo \gls{cli}}
    \label{tab:cli_args}
    \begin{tabular}{p{0.13\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|p{0.3\textwidth}}
        \hline
        \textbf{Name}                            & \textbf{Variable Name} & \textbf{Default Value}     & \textbf{Description}                                                                                                      \\ \hline
        \multicolumn{1}{l|}{--help}              &                        & ---                        & Prints list of         arguments with hints                                                                                       \\
        \multicolumn{1}{l|}{-i}                  & i                      & data/input/definition.json & Path to a         definition file that defines the settings                                                                 \\
        \multicolumn{1}{l|}{-v}                  & v                      & False                      & Verbose output,         logging information will also be displayed on STDOUT                                                      \\
        \multicolumn{1}{l|}{--cli}               & cli                    & False                      & If the \gls{cli}       flag is set, an interactive \gls{cli} will be started. NOT IMPLEMENTED. \\
        \multicolumn{1}{l|}{--profile}           & profile                & False                      & If the profile         flag is set, Python's cProfile will be used to profile \gls{sispo} execution              \\
        \multicolumn{1}{l|}{--no-sim}            & with\_sim              & True                       & If flag is set,        simulation step will be skipped                                                                           \\
        \multicolumn{1}{l|}{--no-render}         & with\_render           & True                       & If flag is set,        rendering step will be skipped                                                                            \\
        \multicolumn{1}{l|}{--no-compression}    & with\_compression      & True                       & If flag is set,        compression step will be skipped                                                                          \\
        \multicolumn{1}{l|}{--no-reconstruction} & with\_reconstruction   & True                       & If flag is set,        reconstruction step will be skipped                                                                       \\
        \multicolumn{1}{l|}{--sim-only}          & sim\_only              & False                      & If flag is set,        only simulation step will be done                                                                         \\
        \multicolumn{1}{l|}{--sim-render-only}   & sim\_render\_only      & False                      & If flag is set,        only simulation and rendering will be done                                                                \\
        \multicolumn{1}{l|}{--render-only}       & render\_only           & False                      & If flag is set,        only rendering will be done                                                                               \\
        \multicolumn{1}{l|}{--compress-only}     & compress\_only         & False                      & If flag is set,        only compression will be done                                                                             \\
        \multicolumn{1}{l|}{--reconstruct-only}  & reconstruct\_only      & False                      & If flag is set, only reconstruction will be done       
    \end{tabular}
\end{table}

The default interface is a \textit{definition.json} file which provides all input parameters in the \gls{json} format. An example is given in the repository in the data/input folder. While this example provides the most important settings, there are more settings that can be used. Especially the software packages used for \gls{sfm} have many default settings which can be customised. For an explanation of all parameters, refer to the documentation of the respective software or Python package.

\gls{sispo} can be imported as a Python module. Execution is then started either by sispo.run() or sispo.main(), which are equivalent.

\subsection{Simulation Module}
The simulation module creates photo-realistic images by simulating a realistic trajectory of a \gls{sssb} and a spacecraft using Orekit. This trajectory and attitude data is then used to render four images per frame, one containing only the \gls{sssb}, one where the view is kept at a constant distance from the \gls{sssb}, one calibration reference image and one that renders a realistic star background using the \gls{ucac4}~\cite{zacharias2013fourthUCAC4}. Additionally, the date, the spacecraft position, \gls{sssb} position and their distance is stored as metadata. These images are composed to one image using photometry to calibrate the absolute light intensity of the different images in terms of realistic photon fluxes using the Johnson magnitude system~\cite{Bessell1979UBVRIPhotometry}. The composition process is explained in more detail in Section~\ref{sec:composition}.

The \textit{sim} sub-package was developed to contain all general information about the environment in the Environment class in order to have one consistent instance for constants and parameters. To minimise the loss of information in intermediate steps, all images during the rendering and calibration process use the OpenEXR format. Additionally, rendered images are stored as \gls{png} files to allow a quick preview of the images. The most data is produced as all Blender scenes are being stored in each time step. This is done to allow later in-depth analysis of the raw Blender scenes, in case there are problems during the rendering process.

A run of the simulation package includes two steps. The first step is propagation, image rendering in the second step.
In the propagate step, Orekit is used to determine state information of the \gls{sssb} and the spacecraft. The state information includes the date, position and the rotation angles of the \gls{sssb}. Propagation is defined by start and end date, number of steps, the timesampler mode and a slowmotion factor.
The timesampler mode determines whether the steps are distributed linear in time (mode 1, default) or whether an exponential model (mode 2) is used which takes more samples around the encounter. How many samples more are taken can be controlled with the slowmotion factor. Mode 2 is especially helpful when simulating a long period since far from the \gls{sssb} nucleus, there are only minor visible changes over longer periods.

The Orekit library runs a \gls{vm} to execute its Java code. Additionally, physical data is required which is currently distributed with the \textit{sim} sub-package. The \gls{vm} and physical data are initialised in the \textit{sim} main module and only afterwards other modules are imported which is why other modules do not include the Orekit initialisation. This approach was taken in order to reduce resource consumption by not having several instances of the \gls{vm} running. Propagation is based on Orekit's KeplerianOrbit and KeplerianPropagator classes. Thus the input required to define the \gls{sssb} trajectory are as presented in Table~\ref{tab:keplerorbit_params}.

\begin{table}[htb]
    \centering
    \caption{Parameters that define a \gls{sssb} orbit.}
    \label{tab:keplerorbit_params}
    \begin{tabular}{p{0.45\textwidth}|p{0.2\textwidth}}
        \textbf{Name}    & \textbf{Unit} \\ \hline
        Semi\-major axis a & \SI{}{\astronomicalunit} \\
        Eccentricity e & \SI{}{} \\
        Inclination i & \SI{}{\radian} \\
        Argument of perigee  $\omega$ & \SI{}{\radian}  \\
        Right ascension of ascending node $\Omega$ & \SI{}{\radian} \\
        Mean anomaly $M$ & \SI{}{\radian}\\
        Date of the orbital parameters &  
    \end{tabular}
\end{table}

The initial state of the spacecraft is calculated based on the input parameters presented in Table~\ref{tab:sc_enc_paras}. Only the three first parameters need to be given as input in the definition file. The \textit{sssb\_state} is calculated based on the \gls{sssb} input data. Additional input parameters that define the simulation are given in Table~\ref{tab:sim_input_params}.

\begin{table}[htb]
    \centering
    \caption{Parameters that define the encounter state of the spacecraft.}
    \label{tab:sc_enc_paras}
    \begin{tabular}{p{0.23\textwidth}|p{0.06\textwidth}|p{0.62\textwidth}}
        \textbf{Parameter}  & \textbf{Type} & \textbf{Description} \\ \hline
        encounter\_distance & float         & Minimum distance between SSSB and spacecraft, in \SI{}{\meter}. \\
        with\_terminator    & bool          & Determines whether the terminator is visible at the closest approach. \\
        with\_sunnyside     & bool          & Determines whether the spacecraft passes the SSSB on the Sun facing side or the SSSB's side facing away from the Sun. \\
        relative\_velocity  & float         & Relative velocity of the spacecraft to the \gls{sssb} at the encounter, in \SI{}{\meter\per\second}. \\
        sssb\_state         & tuple         & \gls{sssb} state vector, including 3 position and 3 velocity components, at encounter. The spacecraft encounter state is calculated relative to this state vector. The \gls{sssb} state does not need to be set in the  definition file but is calculated based on the \gls{sssb} trajectory and encounter date.
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Additional input parameters for simulation.}
    \label{tab:sim_input_params}
    \begin{tabular}{p{0.23\textwidth}|p{0.06\textwidth}|p{0.62\textwidth}}
        \textbf{Parameter}  & \textbf{Type} & \textbf{Description} \\ \hline
        duration            & float         & Total length of the simulation, in \SI{}{\second}. The closest approach is reached at half of the duration. \\
        timesampler\_mode   & int           & Mode 1 = linear, mode 2 = exponential sampling.  \\
        slowmotion\_factor  & float         & Determines how many more state samples are taken around the encounter. Only applies if timesampler\_mode is exponential. \\
    \end{tabular}
\end{table}

For rendering it is necessary to provide \gls{3d} models to the simulation. Since Blender is used, the path to ".blend" files needs to be given. Within a given ".blend" file, the object is identified by its name.

Additionally, several parameters to describe the instrument are required to produce realistic output. The parameters are given in Table~\ref{tab:inst_input}. The current implementation of rendering and compositing assumes an instrument with \gls{rgb} channels.

\begin{table}[htb]
    \centering
    \caption{Input parameters to describe an instrument.}
    \label{tab:inst_input}
    \begin{tabular}{p{0.17\textwidth}|p{0.16\textwidth}|p{0.05\textwidth}|p{0.5\textwidth}}
        \textbf{Name} & \textbf{Parameter} & \textbf{Unit} & \textbf{Description} \\ \hline
        Resolution & res & pixel & Number of pixels of the \gls{ccd} in x- and y-axis. Two values need to be given as either list or tuple.\\
        Pixel length & pixel\_l & \SI{}{\micro\meter} & Length of a pixel of the \gls{ccd} .\\
        Focal length & focal\_l & \SI{}{\milli\meter} & Focal length of the optical bench. \\
        Aperture diameter & aperture\_d & \SI{}{\centi\meter} & Diameter of the aperture of the optical bench.\\
        Wavelength & wavelength & \SI{}{\nano\meter} & Center wavelength of the instrument. \\
        Overall optical efficiency & quantum\_eff & & Optical efficiency of the optical system, including optical bench, quantum efficiency of the \gls{ccd}. \\
        Color depth & color\_depth & \SI{}{\bit} & Bit depth of the \gls{ccd}. 
    \end{tabular}
\end{table}


\subsubsection{Blender}
To render the \gls{sssb} and calibration images, Blender is used. A set of predefined settings is used for each scene while some parameters can be changed via the definition file. The rendering engine \textit{Cycles} is used to render photo-realistic, physics-based images from the Blender scenes.

Blender is interfaced using its Python bindings which need to be compiled manually (cf. Appendix~\ref{sec:app_setup} for setup instructions). To control the behaviour of Blender the BlenderController class is used. It handles setting all defaults such as a black background and the creation of the empty default scene, SssbOnly. In addition, the BlenderController interfaces to the star catalogue and the compositor.

Since the images are composed and calibrated at a later step, described in Section~\ref{sec:composition}, the settings shown in Table~\ref{tab:color_space} are selected to not change the rendered raw images with Blender while saving.

The interface of the BlenderController class is usable with a scene name (string), a list of scene names (list of strings), a scene object (bpy.types.Scene) or a list of scene objects (list of bpy.types.Scene). This provides flexibility when using the BlenderController class in the simulation module.

\begin{table}[htb]
    \centering
    \caption{Blender settings relevant to color space and are fixed to get raw images out of the Blender rendering process.}
    \label{tab:color_space}
    \begin{tabular}{p{0.25\textwidth}|p{0.08\textwidth}|p{0.57\textwidth}}
        \textbf{Name}        & \textbf{Value} & \textbf{Description} \\ \hline
        Color mode           & RGBA           & Use RGB and alpha channel. \\
        Sequencer colorspace & Raw            & Color space sequencer uses linear color space. \\
        View transform       & Raw            & No color space conversion. \\
        Look                 & None           & No artistic effect is applied before color space conversion. \\
        Film transparent     & True           & World background is transparent, i.e. alpha values can be used for image composition and proper occultation.
    \end{tabular}
\end{table}

\begin{table}[htb]
    \centering
    \caption{Blender settings that can be changed.}
    \label{tab:blender_settings_input}
        \begin{tabular}{p{0.12\textwidth}|p{0.15\textwidth}|p{0.62\textwidth}}
        \textbf{Name}       & \textbf{Default Value} & \textbf{Description} \\ \hline
        device     & AUTO          & Device used for rendering. "AUTO" attempts to use GPU, if not available uses CPU. \\
        samples    & 6             & Number of samples rendered per pixel.\\
        exposure   & 0             & Exposure in stops, applied before display transform. \\
        tile\_size & 512 (GPU)/ 128 (CPU)       & Size of a rendering tile. When rendering with a \gls{gpu} a bigger tile size tends to have better performance. When rendering with a \gls{cpu}, a smaller tile size tends to have better performance.\footnote{\url{https://docs.blender.org/manual/en/latest/render/cycles/render\_settings/performance.html?highlight=tile\%20size}}
    \end{tabular}
\end{table}

Within a Blender scene, the \gls{sssb} is kept at the origin. Only the Sun object and the cameras change position. In total there are three scenes with one camera each. These scenes are called \textit{SssbOnly} with the \textit{ScCam} camera, the \textit{SssbConstDist} scene with the \textit{SssbConstDistCam} and the \textit{LightRef} scene with the \textit{LightRefCam} camera. Each camera is configured equally, using the Instrument data from the input file. This is necessary for the composition step. All cameras are perspective cameras with a distance of $[\SI{1e-5}{},\SI{1e32}{}]$. The \gls{fov} of the \textit{SssbOnly} camera is also used for rendering stars (cf. \ref{sec:stars}).

Due to problems during rendering, the Blender scene is scaled to kilometres instead of metres. I.e. one Blender unit corresponds to \SI{1}{\kilo\meter}. This is the only deviation from SI base units within the software.

The precise shape and surface details of the \gls{sssb} are not provided as texture files and exact shape models. The raw Blender model is a smooth body without any textures as shown in Figure~\ref{fig:smooth_model}. Blender shader nodes are used to create the surface shape and texture during rendering. Blender nodes are a method of programming complex algorithms graphically. These shader nodes are combined in a network or tree. An overview of the shader node network used in our rendering process can be seen in Figure~\ref{fig:shader_nodes}. The network consists of two main trees, one for shading and one for displacement, which are interlinked, i.e. displacement influences surface colour and vice-versa. This approach of procedural terrain generation allows to use the same model on a large range of distances since the details are generated as needed.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.4\textwidth]{doc/thesis/0_figures/procedural_terrain/smooth_model.png}
    \caption{Smooth model of a \gls{sssb}. This model shows the raw \gls{3d} model before applying surface and displacement shaders.}
    \label{fig:smooth_model}
\end{figure}

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/procedural_terrain/node_network.png}
    \caption{Overview of the shader node network in the Blender model used for creating a \gls{sssb} surface and terrain. Two main trees are visible in the network that are combined in the output, one for shading and the other for displacement. The two main trees are interlinked.}
    \label{fig:shader_nodes}
\end{figure}

The shader implementation creates a material that combines surface shading and displacement. The used nodes are grouped into five categories. Math nodes, texture nodes, colouring nodes, vector nodes and shader nodes. A detailed description can be found in the Blender manual, we provide only the most relevant information for our implementation~\cite{IntroductionManual}.

Math nodes include mathematical operators such as add, subtract, multiply, greater than or maximum of a value. Vector nodes allow transformations in \gls{3d} space such as translation, scaling or displacement. We use the Displacement node to move the surface along the surface normals. Colouring nodes are used to change colours. Our model uses the RGB Curves node which allows manipulation of colours separated by channel using a curve that maps the input to output values. Also Mix nodes are used to combine colours. Texture nodes provide procedural creation of texture by calculating colours based on mathematical functions and model coordinates. The Noise Texture node is used to create textured based on Perlin noise. The Voronoi Texture node creates textures using Voronoi patterns, or Worely noise. Three shader nodes are used in our model, the Principled \gls{bsdf}, Diffuse \gls{bsdf} and the Mix Shader node. The Principled \gls{bsdf} node combines many shading features in a single node. The \gls{bsdf} at the output includes a mix of sheen tint, surface roughness and index of refraction among others. The Diffuse BSDF node provides diffuse reflection using a Lambertian and Oren-Nayar model. The Mix Shader node combines its input shaders with a given probability for using either of the input shaders as its output.
%math nodes: add, subtract, multiply, greater than, maximum 
%vector: displacement
%colouring: RGB curves, Mix
%texture node: noise, Voronoi
%shader: Principled BSDF, Diffuse BSDF, mix

\subsubsection{Star Rendering} \label{sec:stars}
The background stars are rendered based on the star catalogue \gls{ucac4}. This catalogue includes stars up to magnitude 16. The \gls{fov} is determined based on the spacecraft camera in the SssbOnly scene since this camera provides the \gls{fov} of the spacecraft instrument. The edges of the \gls{fov} are calculated using Equation \ref{eq:fov_edge}. 
In the next step, we retrieve the list of stars that are visible in the \gls{fov}. The interface for the \gls{ucac4} is a commandline tool u4test.exe~\cite{gray}. This interface however requires that the \gls{fov} is input as right ascension $\alpha$, declination $\delta$ and width and height.
The conversion of the vectors into their respective right ascension and declination is a transform between Cartesian coordinates to spherical coordinates. Their relation is defined as
\begin{align}
    \delta = \arcsin{(z)}, \label{eq:declination} \\
    \alpha = \arccos{\left(\frac{x}{\cos{\delta}}\right)} + \pi, \label{eq:right_ascension}
\end{align}
where $\delta$ is the declination, $\alpha$ the right ascension, $x$ is the x-coordinate and $y$ is the y-coordinate. Using this information the star catalogue returns a list of visible stars with their right ascension and declination as well as their magnitude.

The star background is created by generating an image with four times the size of final image. The coordinates are converted from the right ascension and declination of each star into pixel coordinates of the image frame using Equation \ref{eq:pix_conversion}. The pixel coordinates are multiplied with two to fit the oversized image.

The pixel value of each colour channel is set to the flux calculated from the magnitude as defined in Equation \ref{eq:mag_flux}. The resulting image is Gaussian filtered and down-sampled to the final image size using local means (cf. Section~\ref{sec:t_downsample}). Star background images are stored in the OpenEXR format with all alpha channel values equal to one. An example image is presented in Fig. \ref{fig:star_rendering}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/star_rendering/Stars_2017-08-15T115856-171000.png}
    \caption{Star background rendering.}
    \label{fig:star_rendering}
\end{figure}

\subsubsection{Composition} \label{sec:composition}
Rendered images of Blender vary in absolute light intensity thus requiring calibration. Since the star maps are not rendered in Blender, the raw images need to be combined. Optionally, the combined images can be transformed to the bit depth of a \gls{ccd}. Hence, the composition process has two steps and an optional third step. 

First, the \gls{sssb} image and the star background are calibrated. In the second step, these images are combined to create a \gls{sssb} image with star background where stars are occluded by the \gls{sssb} nucleus. For calibration, the Blender output is normalised by comparing the theoretical and rendered intensity of a calibration disk in the same conditions.

The starting point for composition is a set of four images with the prefixes SssbOnly, SssbConstDist, LightRef and Stars. An example set is shown in Figure~\ref{fig:comp_imageset}. SssbOnly represents the view from the spacecraft instrument. SssbConstDist is a follower camera at a constant distance of \SI{1000}{\kilo\meter} from the \gls{sssb}. LightRef is a defined reference model. Stars is an image of the star background. These images are then composed to a single final image.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/SssbOnly_2017-08-15T115855-684000.png}
        \caption{SssbOnly}
        \label{fig:comp_sssbonly}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/SssbConstDist_2017-08-15T115855-684000.png}
        \caption{SssbConstDist}
        \label{fig:comp_sssbconstdist}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/LightRef_2017-08-15T115855-684000.png}
        \caption{LightRef}
        \label{fig:comp_lightref}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/Stars_2017-08-15T115855-684000.png}
        \caption{Stars}
        \label{fig:comp_stars}
    \end{subfigure}
    \caption{Example set of images used for composition to the final rendering output.}
    \label{fig:comp_imageset}
\end{figure}

Photometric calibration is based on the V-band of the \gls{ubv} as described in Section~\ref{sec:photo_cal}. The calibration reference is the solar photon flux in the V-band. The Sun has a magnitude $m_{\odot} = -26.74$ at a distance of \SI{1}{\astronomicalunit}. Using Equation \ref{eq:comp_flux_0mag} and the constants for the V-band from Table~\ref{tab:ubv_constants}, the solar photon flux density at \SI{1}{\astronomicalunit} becomes \SI{4.36715206e+20}{\per\second\per\square\meter}.
 
Since the spacecraft is normally not at exactly \SI{1}{\astronomicalunit}, this flux density is scaled using an inverse square law defined as
\begin{align}
    F_d = F \times \left(\frac{\SI{1}{\astronomicalunit}}{d}\right)^2, \label{eq:inverse_square}
\end{align}
where $F_d$ is the scaled photon flux density, $F$ is the flux density at \SI{1}{\astronomicalunit} and $d$ is the spacecraft's distance from the Sun in \SI{}{\astronomicalunit}.

In the next step, the reference photon flux for one pixel is calculated using Equation \ref{eq:comp_flux_pix} and the scaled photon flux density $F_d$.

To obtain the calibration factor for each pixel of the \gls{sssb} image, Equation \ref{eq:comp_cal_fac} is used. The reference intensity of the image is calculated as the mean intensity of $\SI{70}{}\times\SI{70}{}$ pixels of the centre of the light reference image.

In case the rendered image is far away from the \gls{sssb} nucleus, the visible size of the \gls{sssb} on the image is too small to be used for calibration. In such a case, a point source \gls{sssb} image is generated and used for calibration along with the SssbConstDist image. The point source is necessary if the \gls{sssb}'s nucleus is only a few pixels of the image, since then intensities are not rendered correctly. The point source image is created by Gaussian filtering a single white pixel at the centre of an oversized image. The image is oversized by a factor of five, and then down-sampled to the original size using local means as described in Section~\ref{sec:t_downsample}. In this case the l

For the star background, every pixel value $v$ is calibrated using
\begin{align}
        v = v_0 \times F_0 \times A \times \frac{F_{stars}}{S_{stars}}, \label{eq:comp_cal_starmap}
\end{align}
with $v_0$ being the original pixel value, $F_0$ being the flux at a magnitude $m = 0$, $A$ being the aperture area, $F_{stars}$ being the total flux of visible stars, and $S_{stars}$ being the summed pixel value of one channel of a star map.

The star background and the calibrated \gls{sssb} image are merged taking the alpha channels into account. Using the alpha channel information provides proper occultation of the star background by the nucleus. For this, the \textit{film\_transparent} option of Blender is activated which makes the scene background transparent, i.e. alpha values are zero.

The composed image is then multiplied by the efficiency of the instrument. This efficiency includes all efficiencies to convert incoming photon flux into pixel value, not only the \gls{ccd} quantum efficiency. Additionally, a diffraction pattern is added as well as noise which is based on a Poisson distribution. The diffraction pattern can be approximated with a Gaussian filter. The standard deviation for the Gaussian approximation of a diffraction pattern was found to be $\sigma \approx 0.45 \times \lambda \times \frac{f}{D}$ when the Gaussian profile should contain the same energy as the diffraction pattern~\cite{Zhang2007GaussianModels}. Using this result, the standard deviation $\sigma$ for Gaussian filtering is defined as
\begin{align}
    \sigma = 0.45 \times \lambda \times \frac{f}{D} \times \frac{m}{l_{pix}}, \label{eq:comp_sigma}
\end{align}
where $\lambda$ is the observed wavelength, $f$ is the focal length, $D$ is the aperture diameter, $l_{pix}$ is the length of one side of a pixel and $m$ is a multiplier for systems that are not diffraction limited (in our case $m = 2$, a perfectly diffraction limited system would have $m = 1$). The noisy image is scaled to an interval $[0,1]$ by dividing through the maximum value. In case a point source \gls{sssb} is used, the maximum value is clipped to five times the maximum of the reference if the maximum value of the merged image is above this threshold. The composed image using the data set shown in Figure~\ref{fig:comp_imageset} is presented Figure~\ref{fig:comp_composed}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/Comp_2017-08-15T115854-575000.png}
    \caption{Composed image of the four images shown in Fig. \ref{fig:comp_imageset}}
    \label{fig:comp_composed}
\end{figure}

In the additional third step, the image values are scaled to the bit depth of the \gls{ccd}. The current maximum permissible bit depth is \SI{16}{bit} due to an increase in code complexity for higher bit depths. This feature can be turned off by setting the \textit{with\_clipping} parameter to false. This feature is used by default, since the aim is to realistically simulate the imaging capabilities of a spacecraft.

An additional feature is to add an information box in the lower right corner of the image including the distance to the \gls{sssb} and the date. This feature can be turned off by setting the \textit{with\_infobox} parameter to false.

\subsection{Compression Module}
The compression module provides compression and decompression algorithms. These can be tested against different mission scenarios and image series to investigate the impact of compression and decompression on the scientific quality of the images. Table~\ref{tab:compression_format} shows the available compression and decompression algorithms.

\begin{table}[htb]
    \centering
    \caption{Lossless and lossy compression algorithms provided within \gls{sispo}.}
    \label{tab:compression_format}
    \begin{tabular}{l|l}
        \textbf{Lossless Format} & \textbf{Lossy Formats} \\ \hline
        \gls{png}              & \gls{jpeg}           \\
        exr               & jpeg200        \\
        bz2               & tiff           \\
        gzip              &                \\
        lzma              &                \\
        zlib              &               
    \end{tabular}
\end{table}

Independent of the algorithm, files are handled equally. The images are loaded into \textit{NumPy} arrays inside Python to have a common raw format. Before encoding, the information is reduced to \SI{8}{\bit} since the reconstruction module only reads images with \SI{8}{\bit} colour depth correctly. Images are encoded with the respective algorithm and converted into a binary stream. A copy of this data is saved as a file in the raw folder. The raw binary data is then converted back to a \textit{NumPy} array and decoded to retrieve as much information as possible. Finally, the compressed and decompressed data is stored as a \gls{png} file to not lose any further information.

The formats \gls{png}, \gls{jpeg}, \gls{jp2} and tiff are implemented using \textit{OpenCV}. There are various settings for each format\footnote{A description can be found at \url{https://docs.opencv.org/4.1.2/d4/da8/group__imgcodecs.html\#ga292d81be8d76901bff7988d18d2b42ac}}.

The most important input parameter that is relevant to all algorithms is the "level" argument in "settings" which describes either the compression level (from 1 to 9 for bz2, gzip, lzma, zlib and \gls{png}, higher meaning more compression) or the quality level (from 1 to 10 for \gls{jpeg} and 0.01 to 10 for \gls{jp2}, higher meaning better quality and less compression). Other input parameters to the compression package are presented in Table~\ref{tab:compression_settings}.

\begin{table}[htb]
    \centering
    \caption{Input parameters of compression package.}
    \label{tab:compression_settings}
    \begin{tabular}{p{0.1\textwidth}|p{0.14\textwidth}|p{0.65\textwidth}}
        \textbf{Name} & \textbf{Default Value} & \textbf{Description} \\ \hline
        algo    & \gls{png}          & Compression algorithm, for a list of available algorithms see Table~\ref{tab:compression_format}\\
        settings    & \{"level": 9\} & Python dictionary including all settings for compression algorithms. For a description of the possible settings see the code or documentation of the providing library. \\
        img\_ext    & \gls{png}          & Image extension to search for when loading images for compression. 
    \end{tabular}
\end{table}

The compression and decompression has a simple implementation of multi-threading to run several image compression and decompression runs concurrently to reduce execution time. Images are only loaded during the compression and decompression process and are removed from memory directly afterwards.

\subsection{Reconstruction Module}
The reconstruction module can be used to generate a \gls{3d} model of an object using a series of images. It provides a full Multi-View Stereo reconstruction data process pipeline. To achieve this, two libraries are used and combined and called using python. The first library is \gls{omvg}~\cite{openMVG}. The second library is \gls{omvs}~\cite{openMVS}. 
The common steps for the complete pipeline is comprised of the following steps:
\begin{enumerate}
    \item Read in images [ImageListing in \gls{omvg}]
    \item Compute visual features [ComputeFeatures in \gls{omvg}]
    \item Match computed features between different images [MatchFeatures in \gls{omvg}]
    \item Generate point cloud from matched features [IncrementalSfM in \gls{omvg}]
    \item Export to \gls{omvs} format [openMVG2openMVS in \gls{omvg}]
    \item Increase number of points in point cloud [DensifyPointCloud in \gls{omvs}]
    \item Create a mesh from the point cloud [ReconstructMesh in \gls{omvs}]
    \item Refine the generated mesh [RefineMesh in \gls{omvs}]
    \item Apply texture to mesh to create final \gls{3d} model [TextureMesh in \gls{omvs}]
\end{enumerate}

\gls{omvg} and \gls{omvs} can be controlled with numerous parameters. All parameters have default values in \gls{sispo} itself which sometimes differ from the default settings of the software packages. These were adapted since they provide better results in the relevant scenarios.

During the image listing step, the spacecraft positions from the simulations are added as motion priors for improving the stability of the \gls{sfm} algorithms.

The reconstruction is based on two incremental and one global \gls{sfm} approach that \gls{omvg} provides. All three algorithms are executed and the number of reconstructed points are compared. The result containing the most points is exported to the \gls{omvs} format for further processing. None of the residuals are taken into account which is why this selection method might choose the highest quality point cloud due to a possible high number of outliers. Hence, if the result of the reconstruction is considered bad, it can be attempted to use one of the other results.

The algorithm used for reconstruction by IncrementalSfM2 is non-deterministic, i.e. results differ between runs on the same data sets. Hence, IncrementalSfM2 should be executed several times and the best result should be selected~\cite{Pajusalu2019CharacterizationMapping}.

\subsection{Performance}
\subsubsection{Image processing benchmark} \label{sec:cvskimage}
The original codebase used the \gls{skimage} and OpenCV libraries in concurrently. In order to reduce the number of dependencies, the relevant functions of the two libraries were benchmarked. For this comparison, OpenCV functions were used to create the same behaviour as the respective \gls{skimage} function. The benchmark compares the performance of the Gaussian filter and a special case of image resizing using local means. A set of five images is selected and are shown in Appendix \ref{sec:app_cvskimage}. Two star images were selected due to the large variance in the number of visible stars. The selected images represent extreme cases with 1804 (Stars1) and 51338 stars (Stars2).

Two computers are used for the benchmark. A laptop with \SI{8}{\giga\byte} \gls{ram}, an Intel\textsuperscript{\textregistered}~Core\texttrademark~i7-6700HQ with \SI{4}{} cores at \SI{2.6}{\giga\hertz} and Windows 10. The second is a workstation computer with \SI{16}{\giga\byte} of \gls{ram}, an Intel\textsuperscript{\textregistered}~Core\texttrademark~i7-8700 processor with \SI{6}{} cores at \SI{3.2}{\giga\hertz} and Ubuntu 18.04.3 LTS.

To compare the performance between the two libraries the ratio of execution time is used. The ratio is defined as
\begin{align}
    Ratio = \frac{t_{skimage}}{t_{opencv}}, \label{eq:bm_exec_ratio}
\end{align}
where $t_{skimage}$ is the execution time of \gls{skimage} and $t_{opencv}$ is the execution time of OpenCV. Each command is executed and timed for 1000 times. The lowest value is chosen as the result, since higher values are rather influenced by other processes running on the respective machine than the relevant code snippet itself~\cite{timeit2020}.

Figure~\ref{fig:bm_comparison} shows the execution time ratios with their averages over set of images. A ratio larger than one corresponds to a longer execution time of \gls{skimage}. OpenCV outperforms \gls{skimage} for both tests on both computers. The maximum absolute difference between pixel values of images is \SI{1.486e-6}{} and \SI{7.153e-7}{} for the Gaussian filtered and the resized images respectively. Such differences are irrelevant considering \gls{ccd} sensor color depths of \SI{16}{bit}.

%Maximum error gauss:  1.4864218655930017e-06
%Maximum error resize:  7.152557373046875e-07

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/cv_skimage/Comparison_Gaussian}
        \caption{Gaussian filtering.}
        \label{fig:bm_comparison_gauss}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/cv_skimage/Comparison_Resize}
        \caption{Resizing.}
        \label{fig:bm_comparison_res}
    \end{subfigure}
    \caption{Comparison of execution time ratios of Gaussian filtering and resizing five images using OpenCV and \gls{skimage} on two computers. Values $> 1$ correspond to OpenCV being faster. The mean values are given in the legend.}
    \label{fig:bm_comparison}
\end{figure}

For our case, OpenCV has a clear performance advantage over the \gls{skimage} library, hence only OpenCV is being used. In addition to the performance advantages, OpenCV might also be used to replace the OpenEXR dependency in the future, if OpenCV's OpenEXR implementation includes alpha channel support\footnote{A GitHub issue was created at \url{https://github.com/YgabrielsY/sispo/issues/128} that links to the relevant OpenCV GitHub issue.}.