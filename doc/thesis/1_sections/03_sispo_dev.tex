\section{\Acrlong{sispo}} \label{sec:sispo}

\gls{sispo} is a software package developed in python. It is separated in different sub-packages. The two main sub-packages are the \textit{sim} and the \textit{reconstruction} package. The third sub-package provides several image compression algorithms to test effects on reconstruction.

The software package is hosted on GitHub using a git version control system. Furthermore, the GitHub project management tools are used, including automated KanBan based projects, issue tracking and pull requests.

The most important python dependencies of \gls{sispo} are:
\begin{itemize}
    \item astropy: Astronomy package developed by \cite{robitaille2013astropy} and \cite{price2018astropy}
    \item Blender: 3D creation suite \cite{blender}
    \item \textit{NumPy}: Scientific computing for python \cite{oliphant2006guideNumPy}
    \item opencv: Computer vision library used for image processing \cite{opencv_library}
    \item OpenEXR: \gls{hdr} image reading and writing \cite{openexr}
    \item Orekit: Space dynamics library \cite{orekit}
\end{itemize}

It was attempted to reduce dependencies to other libraries as much as possible. Originally, both \textit{scikit-image} and \textit{opencv} were used. After a small benchmark between the two libraries, it was evident that \textit{opencv} performs three to seven times faster compared to \textit{scikit-image} (cf. \ref{sec:cvskimage}. Hence \textit{scikit-image} was completely replaced with equivalent \textit{opencv} functions. Additionally, it was attempted to use the \textit{opencv} package to replace the \textit{OpenEXR} dependency since it is not easy to install. However, this is currently not possible as the \textit{OpenEXR} implementation of \textit{opencv} does not provide an alpha channel and is generally less flexible.

To ease development, numerous parameters are silently assigned default values if not provided e.g. for an instrument. Therefore, all parameters should be explicitly set before running a simulation.

\gls{sispo} was developed in an attempt to logically separate the distinct steps involved of the entire processing pipeline. Such modularity eases further development as well as reduce unnecessary library imports to reduce memory consumption.

\subsection{User Interface}
The \gls{sispo} package can be installed using pip and the GitHub project. If done, it will be installed into the site-packages folder of the used python environment. Additionally, an executable is installed, providing a \gls{cli} as user interface. The most recent possible input arguments are documented in the repository or by using the --help input. The most important arguments for the \gls{cli} are seen in Table \ref{tab:cli_args}.

\begin{table}[htpb]
    \centering
    \caption{Input arguments for sispo \gls{cli}}
    \label{tab:cli_args}
    \begin{tabular}{p{0.13\textwidth}|p{0.15\textwidth}|p{0.15\textwidth}|p{0.3\textwidth}}
        \hline
        \textbf{Name}                            & \textbf{Variable Name} & \textbf{Default Value}     & \textbf{Description}                                                                                                      \\ \hline
        \multicolumn{1}{l|}{--help}              &                        & ---                        & Prints list of         arguments with hints                                                                                       \\
        \multicolumn{1}{l|}{-i}                  & i                      & data/input/definition.json & Path to a         definition file that defines the settings                                                                 \\
        \multicolumn{1}{l|}{-v}                  & v                      & False                      & Verbose output,         logging information will also be displayed on STDOUT                                                      \\
        \multicolumn{1}{l|}{--cli}               & cli                    & False                      & If the \gls{cli}       flag is set, an interactive \gls{cli} will be started. NOT IMPLEMENTED. \\
        \multicolumn{1}{l|}{--profile}           & profile                & False                      & If the profile         flag is set, Python's cProfile will be used to profile \gls{sispo} execution              \\
        \multicolumn{1}{l|}{--no-sim}            & with\_sim              & True                       & If flag is set,        simulation step will be skipped                                                                           \\
        \multicolumn{1}{l|}{--no-render}         & with\_render           & True                       & If flag is set,        rendering step will be skipped                                                                            \\
        \multicolumn{1}{l|}{--no-compression}    & with\_compression      & True                       & If flag is set,        compression step will be skipped                                                                          \\
        \multicolumn{1}{l|}{--no-reconstruction} & with\_reconstruction   & True                       & If flag is set,        reconstruction step will be skipped                                                                       \\
        \multicolumn{1}{l|}{--sim-only}          & sim\_only              & False                      & If flag is set,        only simulation step will be done                                                                         \\
        \multicolumn{1}{l|}{--sim-render-only}   & sim\_render\_only      & False                      & If flag is set,        only simulation and rendering will be done                                                                \\
        \multicolumn{1}{l|}{--render-only}       & render\_only           & False                      & If flag is set,        only rendering will be done                                                                               \\
        \multicolumn{1}{l|}{--compress-only}     & compress\_only         & False                      & If flag is set,        only compression will be done                                                                             \\
        \multicolumn{1}{l|}{--reconstruct-only}  & reconstruct\_only      & False                      & If flag is set, only reconstruction will be done       
    \end{tabular}
\end{table}

The default interface is a \textit{definition.json} file which provides all input parameters in the \gls{json} format. An example is given in the repository in the data/input folder. While this example provides the most important settings, there are more settings that can be used. Especially the software packages used for \gls{sfm} have many default settings which can be customised. For an explanation of all parameters, refer to the documentation of the respective software or Python package.

\gls{sispo} can be imported as a Python module. Execution is then started either by sispo.run() or sispo.main(), which are equivalent.

\subsection{Simulation Module}
The simulation module creates photo-realistic images by simulating a realistic trajectory of a \gls{sssb} and a spacecraft using Orekit. This trajectory and attitude data is then used to render four images per frame, one containing only the \gls{sssb}, one where the view is kept at a constant distance from the \gls{sssb}, one calibration reference image and one that renders a realistic star background using the \gls{ucac4} \cite{zacharias2013fourthUCAC4}. Additionally, the date, the spacecraft position, \gls{sssb} position and their distance is stored as metadata. These images are composed to one image using photometry to calibrate the absolute light intensity of the different images in terms of realistic photon fluxes using the Johnson magnitude system \cite{bessel1979ubvri}. The composition process is explained in more detail in section \ref{sec:composition}.

The \textit{sim} sub-package was developed to contain all general information about the environment in the Environment class in order to have one consistent instance for constants and parameters. To minimise the loss of information in intermediate steps, all images during the rendering and calibration process use the OpenEXR format. Additionally, rendered images are stored as png files to allow a quick preview of the images. The most data is produced as all Blender scenes are being stored in each time step. This is done to allow later in-depth analysis of the raw Blender scenes, in case there are problems during the rendering process.

A run of the simulation package includes two steps. The first step is propagation, image rendering in the second step.
In the propagate step, Orekit is used to determine state information of the \gls{sssb} and the spacecraft. The state information includes the date, position and the rotation angles of the \gls{sssb}. Propagation is defined by start and end date, number of steps, the timesampler mode and a slowmotion factor.
The timesampler mode determines whether the steps are distributed linear in time (mode 1, default) or whether an exponential model (mode 2) is used which takes more samples around the encounter. How many samples more are taken can be controlled with the slowmotion factor. Mode 2 is especially helpful when simulating a long period since far from the \gls{sssb} nucleus, there are only minor visible changes over longer periods.

The Orekit library runs a \gls{vm} to execute its Java code. Additionally, physical data is required which is currently distributed with the \textit{sim} sub-package. The \gls{vm} and physical data are initialised in the \textit{sim} main module and only afterwards other modules are imported which is why other modules do not include the Orekit initialisation. This approach was taken in order to reduce resource consumption by not having several instances of the \gls{vm} running. Propagation is based on Orekit's KeplerianOrbit and KeplerianPropagator classes. Thus the input required to define the \gls{sssb} trajectory are as presented in Table \ref{tab:keplerorbit_params}.

\begin{table}[htpb]
    \centering
    \caption{Parameters that define a \gls{sssb} orbit.}
    \label{tab:keplerorbit_params}
    \begin{tabular}{p{0.45\textwidth}|p{0.2\textwidth}}
        \textbf{Name}    & \textbf{Unit} \\ \hline
        Semi\-major axis a & \SI{}{\astronomicalunit} \\
        Eccentricity e & \SI{}{} \\
        Inclination i & \SI{}{\radian} \\
        Argument of perigee  $\omega$ & \SI{}{\radian}  \\
        Right ascension of ascending node $\Omega$ & \SI{}{\radian} \\
        Mean anomaly $M$ & \SI{}{\radian}\\
        Date of the orbital parameters &  
    \end{tabular}
\end{table}

The initial state of the spacecraft is calculated based on the input parameters presented in Table \ref{tab:sc_enc_paras}. Only the three first parameters need to be given as input in the definition file. The \textit{sssb\_state} is calculated based on the \gls{sssb} input data. Additional input parameters that define the simulation are given in Table \ref{tab:sim_input_params}.

\begin{table}[htpb]
    \centering
    \caption{Parameters that define the encounter state of the spacecraft.}
    \label{tab:sc_enc_paras}
    \begin{tabular}{p{0.23\textwidth}|p{0.06\textwidth}|p{0.62\textwidth}}
        \textbf{Parameter}  & \textbf{Type} & \textbf{Description} \\ \hline
        encounter\_distance & float         & Minimum distance between SSSB and spacecraft, in \SI{}{\meter}. \\
        with\_terminator    & bool          & Determines whether the terminator is visible at the closest approach. \\
        with\_sunnyside     & bool          & Determines whether the spacecraft passes the SSSB on the Sun facing side or the SSSB's side facing away from the Sun. \\
        relative\_velocity  & float         & Relative velocity of the spacecraft to the \gls{sssb} at the encounter, in \SI{}{\meter\per\second}. \\
        sssb\_state         & tuple         & \gls{sssb} state vector, including 3 position and 3 velocity components, at encounter. The spacecraft encounter state is calculated relative to this state vector. The \gls{sssb} state does not need to be set in the  definition file but is calculated based on the \gls{sssb} trajectory and encounter date.
    \end{tabular}
\end{table}

\begin{table}[htpb]
    \centering
    \caption{Additional input parameters for simulation.}
    \label{tab:sim_input_params}
    \begin{tabular}{p{0.23\textwidth}|p{0.06\textwidth}|p{0.62\textwidth}}
        \textbf{Parameter}  & \textbf{Type} & \textbf{Description} \\ \hline
        duration            & float         & Total length of the simulation, in \SI{}{\second}. The closest approach is reached at half of the duration. \\
        timesampler\_mode   & int           & Mode 1 = linear, mode 2 = exponential sampling.  \\
        slowmotion\_factor  & float         & Determines how many more state samples are taken around the encounter. Only applies if timesampler\_mode is exponential. \\
    \end{tabular}
\end{table}

For rendering it is necessary to provide 3D models to the simulation. Since Blender is used, the path to ".blend" files needs to be given. Within a given ".blend" file, the object is identified by its name.

Additionally, several parameters to describe the instrument are required to produce realistic output. The parameters are given in Table \ref{tab:inst_input}. The current implementation of rendering and compositing assumes an instrument with \gls{rgb} channels.

\begin{table}[htpb]
    \centering
    \caption{Input parameters to describe an instrument.}
    \label{tab:inst_input}
    \begin{tabular}{p{0.17\textwidth}|p{0.16\textwidth}|p{0.05\textwidth}|p{0.5\textwidth}}
        \textbf{Name} & \textbf{Parameter} & \textbf{Unit} & \textbf{Description} \\ \hline
        Resolution & res & pixel & Number of pixels of the \gls{ccd} in x- and y-axis. Two values need to be given as either list or tuple.\\
        Pixel length & pixel\_l & \SI{}{\micro\meter} & Length of a pixel of the \gls{ccd} .\\
        Focal length & focal\_l & \SI{}{\milli\meter} & Focal length of the optical bench. \\
        Aperture diameter & aperture\_d & \SI{}{\centi\meter} & Diameter of the aperture of the optical bench.\\
        Wavelength & wavelength & \SI{}{\nano\meter} & Center wavelength of the instrument. \\
        Overall optical efficiency & quantum\_eff & & Optical efficiency of the optical system, including optical bench, quantum efficiency of the \gls{ccd}. \\
        Color depth & color\_depth & \SI{}{\bit} & Bit depth of the \gls{ccd}. 
    \end{tabular}
\end{table}


\subsubsection{Blender}
To render the \gls{sssb} and calibration images, Blender is used. A set of predefined settings is used for each scene while some parameters can be changed via the definition file. The rendering engine \textit{Cycles} is used to render photo-realistic, physics-based images from the Blender scenes.

Blender is interfaced using its Python bindings which need to be compiled manually (cf. \ref{sec:setup}. To control the behaviour of Blender the BlenderController class is used. It handles setting all defaults such as a black background and the creation of the empty default scene, SssbOnly. In addition, the BlenderController interfaces to the star catalogue and the compositor.

Since the images are composed and calibrated at a later step, described in \ref{sec:composition}, the settings shown in Table \ref{tab:color_space} are selected to not change the rendered raw images with Blender while saving.

The interface of the BlenderController class is usable with a scene name (string), a list of scene names (list of strings), a scene object (bpy.types.Scene) or a list of scene objects (list of bpy.types.Scene). This provides flexibility when using the BlenderController class in the simulation module.

\begin{table}[htpb]
    \centering
    \caption{Blender settings relevant to color space and are fixed to get raw images out of the Blender rendering process.}
    \label{tab:color_space}
    \begin{tabular}{p{0.25\textwidth}|p{0.08\textwidth}|p{0.57\textwidth}}
        \textbf{Name}        & \textbf{Value} & \textbf{Description} \\ \hline
        Color mode           & RGBA           & Use RGB and alpha channel. \\
        Sequencer colorspace & Raw            & Color space sequencer uses linear color space. \\
        View transform       & Raw            & No color space conversion. \\
        Look                 & None           & No artistic effect is applied before color space conversion. \\
        Film transparent     & True           & World background is transparent, i.e. alpha values can be used for image composition and proper occultation.
    \end{tabular}
\end{table}

\begin{table}[htpb]
    \centering
    \caption{Blender settings that can be changed.}
    \label{tab:blender_settings_input}
        \begin{tabular}{p{0.12\textwidth}|p{0.15\textwidth}|p{0.62\textwidth}}
        \textbf{Name}       & \textbf{Default Value} & \textbf{Description} \\ \hline
        device     & AUTO          & Device used for rendering. "AUTO" attempts to use GPU, if not available uses CPU. \\
        samples    & 6             & Number of samples rendered per pixel.\\
        exposure   & 0             & Exposure in stops, applied before display transform. \\
        tile\_size & 512 (GPU)/ 128 (CPU)       & Size of a rendering tile. When rendering with a \gls{gpu} a bigger tile size tends to have better performance. When rendering with a \gls{cpu}, a smaller tile size tends to have better performance.\footnote{\url{https://docs.blender.org/manual/en/latest/render/cycles/render\_settings/performance.html?highlight=tile\%20size}}
    \end{tabular}
\end{table}

Within a Blender scene, the \gls{sssb} is kept at the origin. Only the Sun object and the cameras change position. In total there are three scenes with one camera each. These scenes are called \textit{SssbOnly} with the \textit{ScCam} camera, the \textit{SssbConstDist} scene with the \textit{SssbConstDistCam} and the \textit{LightRef} scene with the \textit{LightRefCam} camera. Each camera is configured equally, using the Instrument data from the input file. This is necessary for the composition step. All cameras are perspective cameras with a distance of $[\SI{1e-5}{},\SI{1e32}{}]$. The \gls{fov} of the \textit{SssbOnly} camera is also used for rendering stars (cf. \ref{sec:stars}).

Due to problems during rendering, the Blender scene is scaled to kilometers instead of meters. I.e. one Blender unit corresponds to \SI{1}{\kilo\meter}. This is the only deviation from SI base units within the software.

The precise shape and surface details of the \gls{sssb} are not provided as texture files and exact shape models. A Blender shader is used to create these during rendering. This approach of procedural terrain generation allows to use the same model on a large range of distances since the details are generated as needed. 

DESCRIBE SHADER IMPLEMENTATION

\subsubsection{Star Rendering} \label{sec:stars}
The background stars are rendered based on the star catalogue \gls{ucac4}. This catalogue includes stars up to magnitude 16. The \gls{fov} is determined based on the spacecraft camera in the SssbOnly scene. The edges of the \gls{fov} are calculated using the following equations

\begin{align}
    e_{right} = v_d + \frac{v_r \times s_w}{2 \times f}, \label{eq:edge_right} \\
    e_{left} = v_d - \frac{v_r \times s_w}{2 \times f}, \label{eq:edge_left} \\
    e_{upper} = v_d + \frac{v_u \times s_h}{2 \times f}, \label{eq:edge_upper} \\
    e_{lower} = v_d - \frac{v_u \times s_h}{2 \times f}, \label{eq:edge_lower}
\end{align}
where $e_{i}$ is a vector for the $i^{th}$ edge of the \gls{fov}, $v_d$ is the direction vector of the camera, $v_r$ is the vector pointing right, $s_w$ is the camera sensor width, $s_h$ is the camera sensor height and $f$ is the focal length. These vectors are converted into their respective right ascension and declination using

\begin{align}
    \delta = \arcsin{(z)}, \label{eq:declination} \\
    \alpha = \arccos{\left(\frac{x}{\cos{\delta}}\right)} + \pi, \label{eq:right_ascension}
\end{align}
where $\delta$ is the declination and $\alpha$ the right ascension. The \gls{fov} is then defined as the right ascension and declination of the center point as well as the width and height. Using this information the star catalogue returns a list of visible stars with their right ascension and declination as well as their magnitude.

The star background is created by generating an image with four times the size of final image. The coordinates are converted from the right ascension and declination of each star into pixel coordinates. Afterwards, setting the pixel value of each colour channel to the flux calculated from the magnitude. The resulting image is Gaussian filtered and downscaled to the final image size using local means, i.e. the average value of four pixels is assigned to the final pixel of the image to achieve the final resolution. The alpha values of the entire image are set to one. Star background images are stored in the OpenEXR format. An example image is presented in Fig. \ref{fig:star_rendering}.

\begin{figure}[htpb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/star_rendering/Stars_2017-08-15T115856-171000.png}
    \caption{Star background rendering.}
    \label{fig:star_rendering}
\end{figure}


\subsubsection{Composition} \label{sec:composition}
Rendered images of Blender vary in absolute light intensity thus requiring calibration. For calibration, the Blender output is normalised by comparing the theoretical and real intensity of a calibration disk in the same conditions. This calibration is based on the Johnson magnitude system, also called the UBV photometric system \cite{bessel1979ubvri}. The V-band is used which is centered around \SI{550}{\nano\meter}.

The starting point for composition is a set of four images with the prefixes SssbOnly (cf. Fig. \ref{fig:comp_sssbonly}), SssbConstDist (cf. Fig. \ref{fig:comp_sssbconstdist}), LightRef (cf. Fig. \ref{fig:comp_lightref}) and Stars (cf. Fig. \ref{fig:comp_stars}). SssbOnly represents the view from the instrument. SssbConstDist is a follower camera at a constant distance of \SI{1000}{\kilo\meter} from the \gls{sssb}. LightRef is a defined reference model. Stars is an image of the star background. These images are then composed to a single final image.

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/SssbOnly_2017-08-15T115855-684000.png}
        \caption{SssbOnly}
        \label{fig:comp_sssbonly}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/SssbConstDist_2017-08-15T115855-684000.png}
        \caption{SssbConstDist}
        \label{fig:comp_sssbconstdist}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/LightRef_2017-08-15T115855-684000.png}
        \caption{LightRef}
        \label{fig:comp_lightref}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/Stars_2017-08-15T115855-684000.png}
        \caption{Stars}
        \label{fig:comp_stars}
    \end{subfigure}
    \caption{Example set of images used for composition to the final rendering output.}
    \label{fig:comp_imageset}
\end{figure}

The reference used for photometric calibration is the solar photon flux in the V-band at \SI{1}{\astronomicalunit} calculated based on \cite{wirth}.

The photon flux is defined as,
\begin{align}
    F = F_0 \times \frac{d\lambda}{\lambda} \times 10^{-0.4 \times m}, \label{eq:comp_flux_0mag}
\end{align}
with $F_0$ being the flux at magnitude 0, $\frac{d\lambda}{\lambda}$ is a factor of 0.16 for the V-band, and $m$ is the object's magnitude. The V-band photon flux at magnitude $m = 0$ in SI units is \SI{5.4964e10}{\per\second \per\square\meter}. With a magnitude of $m_{\bigodot} = -26.74$ at a distance of \SI{1}{\astronomicalunit}, the solar photon flux becomes \SI{4.36715206e+20}{\per\second\per\square\meter}.

In the first step, the reference photon flux per pixel is calculated using,
\begin{align}
        F_{ref} = F \times \left(\frac{\SI{1}{\astronomicalunit}}{d_{\bigodot}}\right)^2 \times \frac{A \times pix_A}{f^2 \times \pi}, \label{eq:comp_ref_flux}
\end{align}
where $F$ is the solar flux at \SI{1}{\astronomicalunit}, $d_{\bigodot}$ is the spacecraft distance from the Sun, $A$ is the aperture area, $pix_A$ is the area of a pixel and $f$ is the focal length.

For the star background, every pixel is calibrated according to Eq. \ref{eq:comp_cal_starmap}.
\begin{align}
        pix = pix_0 \times F_0 \times A \times \frac{F_{stars}}{S_{stars}}, \label{eq:comp_cal_starmap}
\end{align}
with $pix_0$ being the original pixel value, $F_0$ being the flux at a magnitude $m = 0$, $A$ being the aperture area, $F_{stars}$ being the total flux of visible stars, and $S_{stars}$ being the summed pixel value of one channel of a star map.

Depending on the visible size of the \gls{sssb} in the image, either a point source \gls{sssb} image is generated and used or the \gls{sssb} image is calibrated. The point source is necessary if the \gls{sssb}'s nucleus is smaller than a pixel outputting wrong intensities.

The point source image is created by Gaussian filtering a single white pixel at the center of an image, oversized by a factor of five, and then downscaled to the original size using local means.

If the \gls{sssb} image is used, the reference intensity of the image is calculated as the mean intensity of $\SI{70}{}\times\SI{70}{}$ pixels of the centre of the light reference image.

Using the calibration factor defined as
\begin{align}
    f_c = \frac{F_{ref} \times I_{ref}}{\alpha}, \label{eq:comp_cal_fac}
\end{align}
where $F_{ref}$ is the reference flux, $I_{ref}$ is the reference intensity and $\alpha$ is the geometric albedo. Every pixel of the image is multiplied with this factor to produce the calibrated \gls{sssb} image. The star background and the calibrated \gls{sssb} image are merged taking the alpha channels into account. Using the alpha channel information provides proper occultation of the star background by the nucleus. For this, the \textit{film\_transparent} option of the Cycles rendering engine is activated.

The composed image is then multiplied by the quantum efficiency of the \gls{ccd}. This quantum efficiency currently includes all efficiencies to convert incoming photon flux into an electrical signal (pixel value). Afterwards, the image is Gaussian filtered and noise based on a Poisson distribution is added. The standard deviation for Gaussian filtering is calculated to approximate the diffraction pattern, defined as
\begin{align}
    \sigma = 0.45 \times \lambda \times \frac{f}{D}\times \frac{1}{pix_l} \times m, \label{eq:comp_sigma}
\end{align}
where $\sigma$ is the standard deviation, $\lambda$ is the instrument wavelength, $f$ is the focal length, $D$ is the aperture diameter, $pix_l$ is the length of one side of a pixel and $m$ is a multiplier for systems that are not diffraction limited (in our case $m = 2$, a perfectly diffraction limited system would have $m = 1$). The noisy image is scaled to an interval $[0,1]$ by dividing through the maximum value. In case a point source \gls{sssb} is used, the maximum value is clipped to five times the maximum of the the reference if the maximum value of the merged image is above this threshold. The composed image using the data set shown in Fig. \ref{fig:comp_imageset} is presented Fig. \ref{fig:comp_composed}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/composition/Comp_2017-08-15T115854-575000.png}
    \caption{Composed image of the four images shown in Fig. \ref{fig:comp_imageset}}
    \label{fig:comp_composed}
\end{figure}

In a final step, the image values are scaled to the bit depth of the \gls{ccd}. The current maximum permissible bit depth is \SI{16}{bit} due to a substantial increase in code complexity for higher bit depths. This feature can be turned off by setting the \textit{with\_clipping} parameter to false.

An additional feature is to add an information box in the lower right corner of the image including the distance to the \gls{sssb} and the date. This feature can be turned off by setting the \textit{with\_infobox} parameter to false.

\subsection{Compression Module}
The compression module provides compression and decompression algorithms. These can be tested against different mission scenarios and image series to investigate the impact of compression and decompression on the scientific quality of the images. Table \ref{tab:compression_format} shows the available compression and decompression algorithms.

\begin{table}[htpb]
    \centering
    \caption{Lossless and lossy compression algorithms provided within \gls{sispo}.}
    \label{tab:compression_format}
    \begin{tabular}{l|l}
        \textbf{Lossless Format} & \textbf{Lossy Formats} \\ \hline
        png               & jpeg           \\
        exr               & jpeg200        \\
        bz2               & tiff           \\
        gzip              &                \\
        lzma              &                \\
        zlib              &               
    \end{tabular}
\end{table}

Independent of the algorithm, files are handled equally to. The images are loaded into \textit{NumPy} arrays inside Python to have a common raw format. Before encoding, the information is reduced to \SI{8}{\bit} since the reconstruction module only reads images with \SI{8}{\bit} colour depth correctly. Images are encoded with the respective algorithm and converted into a binary stream. A copy of this data is saved as a file in the raw folder. The raw binary data is then converted back to a \textit{NumPy} array and decoded to retrieve as much information as possible. Finally, the compressed and decompressed data is stored as a png file to not lose any further information.

The formats png, jpeg, jpeg2000 and tiff are implemented using \textit{OpenCV}. There are various settings for each format\footnote{A description can be found at \url{https://docs.opencv.org/4.1.2/d4/da8/group__imgcodecs.html\#ga292d81be8d76901bff7988d18d2b42ac}}.

The most important input parameter that is relevant to all algorithms is the "level" argument in "settings" which describes either the compression level (from 1 to 9 for bz2, gzip, lzma, zlib and png, higher meaning more compression) or the quality level (from 1 to 10 for jpeg and jpeg2000, higher meaning better quality and less compression). Other input parameters to the compression package are presented in Table \ref{tab:compression_settings}.

\begin{table}[htpb]
    \centering
    \caption{Input parameters of compression package.}
    \label{tab:compression_settings}
    \begin{tabular}{p{0.1\textwidth}|p{0.14\textwidth}|p{0.65\textwidth}}
        \textbf{Name} & \textbf{Default Value} & \textbf{Description} \\ \hline
        algo    & png          & Compression algorithm, for a list of available algorithms see Table \ref{tab:compression_format}\\
        settings    & \{"level": 9\} & Python dictionary including all settings for compression algorithms. For a description of the possible settings see the code or documentation of the providing library. \\
        img\_ext    & png          & Image extension to search for when loading images for compression. 
    \end{tabular}
\end{table}

The compression and decompression has a simple implementation of multi-threading to run several image compression and decompression runs concurrently to reduce execution time. Images are only loaded during the compression and decompression process and are removed from memory directly afterwards.

\subsection{Reconstruction Module}
The reconstruction module can be used to generate a 3D model of an object using a series of images. It provides a full Multi-View Stereo reconstruction data process pipeline. To achieve this, two libraries are used and combined and called using python. The first library is \gls{omvg} \cite{openMVG}. The second library is \gls{omvs} \cite{openMVS}. 
The common steps for the complete pipeline is comprised of the following steps:
\begin{enumerate}
    \item Read in images [ImageListing in \gls{omvg}]
    \item Compute visual features [ComputeFeatures in \gls{omvg}]
    \item Match computed features between different images [MatchFeatures in \gls{omvg}]
    \item Generate point cloud from matched features [IncrementalSfM in \gls{omvg}]
    \item Export to \gls{omvs} format [openMVG2openMVS in \gls{omvg}]
    \item Increase number of points in point cloud [DensifyPointCloud in \gls{omvs}]
    \item Create a mesh from the point cloud [ReconstructMesh in \gls{omvs}]
    \item Refine the generated mesh [RefineMesh in \gls{omvs}]
    \item Apply texture to mesh to create final 3D model [TextureMesh in \gls{omvs}]
\end{enumerate}

\gls{omvg} and \gls{omvs} can be controlled with numerous parameters. All parameters have default values in \gls{sispo} itself which sometimes differ from the default settings of the software packages. These were adapted since they provide better results in the relevant scenarios.

The reconstruction is based on two incremental and one global \gls{sfm} approach that \gls{omvg} provides. All three algorithms are run and the number of reconstructed points are compared. The result containing the most points is exported to the \gls{omvs} format for further processing. None of the residuals are taken into account which is why this selection method might choose the wrong result due to a possible high number of outliers. Hence, if the result of the reconstruction is considered bad, it can be attempted to use one of the other results.

\subsection{Setup} \label{sec:setup}
\gls{sispo} can be setup with Linux and Windows. The default case used in this description is a Windows setup. It is recommended to set \gls{sispo} up in a Windows environment since e.g. the reconstruction algorithms seemed to be more stable. Known differences or problems under Linux will be pointed out. While it should be possible to use a plain Python environment and pip, a miniconda environment manager was used for development. Also a C compiler is necessary. Linux provides the GCC, for Windows it is easiest to install Microsoft Visual Studio with \gls{msvc} and \gls{msbuild}. Another possibility when using Windows is to use vcpkg\footnote{Found at \url{https://github.com/microsoft/vcpkg}}. However, previously the openMVG and openMVS ports in vcpkg did not work. Vcpkg can also be used with Linux. However, there were unsolvable problems when using vcpkg so everything was installed natively.

For \gls{omvg}, \gls{omvs} and star\_cats it is necessary to have the executables in the correct folder for \gls{sispo} to function.\newline

\begin{figure}
    \dirtree{%
        .1 sispo.
            .2 build.
            .2 data.
                .3 input.
                .3 models.
                .3 sensor\_database.
                .3 UCAC4.
                    .4 u4b.
                    .4 u4i.
            .2 doc.
            .2 sispo.
                .3 compression.
                .3 reconstruction.
                .3 sim.
            .2 software.
                .3 blender.
                .3 miniconda.
                .3 openMVG.
                    .4 build\_openMVG.
                        .5 install.
                    .4 openMVG.
                .3 openMVS.
                    .4 build\_openMVS.
                        .5 install.
                    .4 openMVS.
                .3 star\_cats.
                    .4 build\_star\_cats.
                    .4 star\_cats.
                .3 vcpkg.
    }
    \caption{Directory structure after setup. No files are shown.}
    \label{fig:dir_tree}
\end{figure}
Figure \ref{fig:dir_tree} shows the assumed overall folder structure after installation. No sub-folders of the build folder or any files are shown.

To make \gls{sispo} perform well, it is beneficial to install the Nvidia CUDA Toolkit (https://developer.nvidia.com/cuda-downloads) in case an Nvidia graphics card is available.

In the following enumeration, commands intended to be run in a shell are highlighted with a grey box.

\begin{enumerate}
    \item Clone the GitHub repository onto the local machine \\ \shellcmd{git clone https://github.com/YgabrielsY/sispo.git}. The project provides a software folder which is intended to be used to install all following software.
    \item Setup (conda) environment with dependencies (to software/miniconda folder):
    \begin{enumerate}
        \item orekit 9.3.1, the current version 10.0 had issues when attempted to install. Also orekit needs a data package to function, it is distributed with \gls{sispo} in the sim module folder.
        \item astropy
        \item opencv
        \item OpenEXR\footnote{For Windows the pre-compiled package found at \url{https://www.lfd.uci.edu/~gohlke/pythonlibs/\#openexr} needs to be used because the pip or conda version do not work.}
        \item \textit{NumPy}
        \item Python\footnote{During development Python version 3.7 was used.}
    \end{enumerate}{}
    \item (Especially Windows) Install vcpkg to software/vcpkg folder, follow instructions at \url{https://github.com/microsoft/vcpkg}
    \item Install Blender as a python module (bpy)\footnote{During development Blender version 2.8 was used.}
    \begin{enumerate}
        \item Clone Blender git repository to software/blender/blender \\ \shellcmd{git clone git://git.blender.org/blender.git}
        \item Compile target bpy \shellcmd{make bpy}, this works also for Windows through the make.bat file provided with Blender
        \item When available: Activate CUDA in the cmake project and recompile
        \item Install bpy to python environment\footnote{Follow these instructions \url{https://blender.stackexchange.com/questions/117200/how-to-build-blender-as-a-python-module}}
    \end{enumerate}{}
    \item Install OpenMVG, follow instructions at \\ \url{https://github.com/openMVG/openMVG/blob/master/BUILD.md} or look for hints in the OpenMVG install script in the build folder.
    \begin{enumerate}
        \item Install dependencies according to instructions
        \item Clone OpenMVG GitHub repository to software/openMVG/openMVG \shellcmd{git clone --recursive https://github.com/openMVG/openMVG.git}
        \item Build to software/openMVG/build\_openMVG folder
        \item Install to software/openMVG/build\_openMVG/install folder
    \end{enumerate}
    \item Install OpenMVS, follow instructions at \\ \url{https://github.com/cdcseacave/openMVS/wiki/Building} or look at the OpenMVS install script in the build folder for hints.
    \begin{enumerate}
        \item Install dependencies according to instructions
        \item Clone OpenMVS GitHub repository to software/openMVS/openMVS \shellcmd{git clone https://github.com/cdcseacave/openMVS.git}
        \item Build to software/openMVS/build\_openMVS folder
        \item Install to software/openMVS/build\_openMVS/install folder
    \end{enumerate}
    \item Install star\_cats
    \begin{enumerate}
        \item Clone star\_cats GitHub repository to software/star\_cats/star\_cats \\ \shellcmd{git clone https://github.com/Bill-Gray/star\_cats.git}
        \item Build to software/star\_cats/build\_star\_cats \shellcmd{make}
    \end{enumerate}
    \item Download UCAC4 star catalog to data/UCAC4, use either:
    \begin{enumerate}
        \item the build/data/download\_ucac4.sh script
        \item download the folder u4b and u4i directly from \\ \url{http://casdc.china-vo.org/mirror/UCAC/UCAC4/}
    \end{enumerate}
\end{enumerate}{}

\subsection{Performance}
\subsubsection{Image processing benchmark} \label{sec:cvskimage}
The original codebase used the \gls{skimage} and OpenCV libraries. In order to reduce the number of dependencies, the relevant functions of the two libraries were benchmarked. For this comparison, OpenCV functions were used to create the same behaviour as the respective \gls{skimage} function. The benchmark compares the performance of the Gaussian filter and a special case of image resizing using local means. A set of five images is selected and are shown in Appendix \ref{sec:app_cvskimage}. Two star images were selected due to the large variance in the number of visible stars. The selected images represent extreme cases with 1804 (Stars1) and 51338 stars (Stars2).

Two computers are used for the benchmark. A laptop with \SI{8}{\giga\byte} \gls{ram}, an Intel\textregistered~Core\texttrademark~i7-6700HQ with \SI{4}{} cores at \SI{2.6}{\giga\hertz} and Windows 10. The second is a workstation computer with \SI{16}{\giga\byte} of \gls{ram}, an Intel\textregistered~Core\texttrademark~i7-8700 processor with \SI{6}{} cores at \SI{3.2}{\giga\hertz} and Ubuntu 18.04.3 LTS.

To compare the performance between the two libraries the ratio of execution time is used. The ratio is defined as
\begin{align}
    Ratio = \frac{t_{skimage}}{t_{opencv}}, \label{eq:bm_exec_ratio}
\end{align}
where $t_{skimage}$ is the execution time of \gls{skimage} and $t_{opencv}$ is the execution time of OpenCV. Each command is executed and timed for 1000 times. The lowest value is chosen as the result, since higher values are rather influenced by other processes running on the respective machine than the relevant code snippet itself \cite{timeit2020}.

Figure \ref{fig:bm_comparison} shows the execution time ratios with their averages over set of images. A ratio larger than one corresponds to a longer execution time of \gls{skimage}. OpenCV outperforms \gls{skimage} for both tests on both computers. The maximum absolute difference between pixel values of images is \SI{1.486e-6}{} and \SI{7.153e-7}{} for the Gaussian filtered and the resized images respectively. Such differences are irrelevant considering \gls{ccd} sensor color depths of \SI{16}{bit}.

%Maximum error gauss:  1.4864218655930017e-06
%Maximum error resize:  7.152557373046875e-07

\begin{figure}[htb]
    \centering
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/cv_skimage/Comparison_Gaussian}
        \caption{Gaussian filtering.}
        \label{fig:bm_comparison_gauss}
    \end{subfigure}
    \begin{subfigure}[b]{0.47\textwidth}
        \centering
        \includegraphics[width=\textwidth]{doc/thesis/0_figures/cv_skimage/Comparison_Resize}
        \caption{Resizing.}
        \label{fig:bm_comparison_res}
    \end{subfigure}
    \caption{Comparison of execution time ratios of Gaussian filtering and resizing five images using OpenCV and \gls{skimage} on two computers. Values $> 1$ correspond to OpenCV being faster. The mean values are given in the legend.}
    \label{fig:bm_comparison}
\end{figure}

For our case, OpenCV has a clear performance advantage over the \gls{skimage} library, hence only OpenCV is being used. In addition to the performance advantages, OpenCV might also be used to replace the OpenEXR dependency in the future, if OpenCV's OpenEXR implementation includes alpha channel support\footnote{A GitHub issue was created at \url{https://github.com/YgabrielsY/sispo/issues/128} that links to the relevant OpenCV GitHub issue.}.

\subsection{Future Developments}
There are several issues left open within the \gls{sispo} software package. First, there is currently no realistic model of spacecraft attitude motion and control implemented. The camera of the simulation environment is perfectly oriented towards the centre of the \gls{sssb}'s nucleus during the entire simulation. Realistic rotation should cover at least two effects, motion blur due to instantaneous rotation velocities of spacecraft and off-centre pointing due to control inaccuracies. Furthermore, it is necessary to include  image distortions such as astigmatism, bokeh, coma, field curvature, glare.
Currently, \gls{sispo} assumes that an instrument always has \gls{rgb} channels. Since many \gls{ccd}s used in deep space are only \gls{bw} cameras, a possibility to choose either \gls{rgb} or \gls{bw} should be implemented.
Moreover, it is necessary to include a gas and dust environment around the nucleus. From a technical perspective, a proper simulation of the data transmission should be included. For example, a realistic simulation for packet loss. The ultimate goal is to develop a prioritisation algorithm for the images which should prioritise data transmission on packet level.
Moreover, multi-instumrent capability was intended as well as including multiple \gls{sssb}s in the future to allow more complex simulations including e.g. a binary system.
Furthermore, the shader used to create the \gls{sssb} models should be developed further. The interface for it should be included into \gls{sispo} and restricted to values that create reasonable shaped outputs.
An attempt to include a HAPKE model via the synthspace package \url{https://github.com/oknuutti/synthspace} was unsuccessful. However, it would be interesting to compare the results of Blender and the HAPKE model. A HAPKE model is substantially faster while providing less detail though possibly still sufficient for some cases.
To improve computational performance and make image compression and possible data transmission more realistic, image cropping should be added. A substantial part of an image close to a nucleus is black which could be cropped away to reduce the amount of data for the computation and the system itself.

THOUGHTS:\\
-default data set, different compression and reconstruction\\
    -compressions: png, jpg2000 quality 1000, jpg2000 quality 500, jpg2000 quality 100, jpg2000 quality 10\\
-different trajectories: default : 400 km, 100km, 1000km\\
-different lighting situations\\
-different models 100m, 1km, 10km