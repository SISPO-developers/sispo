\section{Theory} \label{sec:theory}
% Physics part
% - Composition (photometry) -> telescope optics?/instrument?/bit depth?
% - Trajectories and relative trajectory of spacecraft, especially trajectories for flybys
% - Star rendering?
% - Describe physics of SSSBs, i.e. size/shapes/surface (features/color/albedo) illumination
%
% Computer Science part
% - Physics-based rendering? -Shaders/procedural terrain generation
% - Compression
% - Reconstruction (SfM), relates to camera physics. Generally Computer Vision topic.
% - Image processing Gaussian filtering, downscale local means
% - Logic for choosing number of reconstructed points as quality measure
%
% Space
% - Small Spacecrafts -> small data budgets?
%
% Max Science?
%
% Concepts to describe???

\subsection{Small Solar System Body}
The \gls{iau} defines a \gls{sssb} as any object in the Solar System, that is not a planet, dwarf planet or satellite \cite{iau_sssb}. Therefore, most asteroids, comets, Trans-Neptunian Objects, minor planets, meteorites and interplanetary dust are included in this definition \cite{wiki:sssb}. This is visualised in Figure \ref{fig:sssb_diagram}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.7\textwidth]{doc/thesis/0_figures/Euler_diagram_of_solar_system_bodies.png}
    \caption{The group of \glspl{sssb} (grey box) include a part of minor planets, trans-Neptunian objects, comets and centaurs \cite{wiki:sssb}.}
    \label{fig:sssb_diagram}
\end{figure}

Within this work, the term \gls{sssb} mostly refers to asteroids and comets. Therefore, general properties of asteroids and comets are explained in more detail.

\subsection{Asteroids}
Asteroids are rocky bodies that mostly reside in an orbit between Mars and Jupiter, i.e. the asteroid main belt. The terms asteroid and minor planet are often used interchangeably. Since most asteroids are only observed remotely, only their ephemeris and absolute magnitude is observed which gives a rough estimate of its size. However, some asteroids also have physical parameters like rotation period, geometric albedo, colours, spectral type, mass and bulk density.

\subsection{Comets}
Comets are small icy bodies and reside mostly in the outer Solar System. On long timescales, comets are perturbed by the gravity of other objects and leave their outer Solar System orbits to get closer to the Sun. In this process, the ices begin to evaporate. This creates the well known coma around the nucleus of a comet. In most cases the coma is several orders of magnitudes larger than the nucleus, the nucleus is on the order of a few kilometres while the coma can be on the order of hundreds of thousands of kilometres. Additionally, two tails are formed, a gas and a dust tail. The gas tail is formed from coma particles that move away from the nucleus and are then carried away by the solar wind. On the other hand, the dust tail is formed from dust particles in the coma which are carried away from the nucleus by the solar radiation pressure.

The physical parameters of comets are not well known, since they are too small for ground observations and it is difficult to image the nucleus when they come closer to the Sun since the nucleus is surrounded by the coma. While the ephemeris and magnitude are known for most comets, only a few comets have known rotation periods and geometric albedos.

\subsection{Computer Vision}
\Gls{cv} is the science of extracting information from digital photos and videos by mimicking the human vision system using a computer. \Gls{cv} encompasses a wide field of activities, from image formation, processing, detecting and matching features, image segmentation and three-dimensional reconstruction \cite{szeliski2010computer}. \Gls{cv} is the computer based variant of photogrammetry which is tasked with obtaining information about physical objects and the environment from photographic images \cite{linder2009digital}. The most common approach for three-dimensional reconstruction is stereo-photogrammetry, also referred to as computer stereo vision when using computers. Stereo-photogrammetry applies the binocular vision principle of the human vision system to obtain structural information from images \cite{do2019review}. Since most, if not all, deep space missions have a visual imager instrument on-board, \gls{cv} constitutes a logical approach of obtaining the three-dimensional structure of an observation target. A similar problem is the \gls{sfm} approach where the motion of the camera creates the different perspective.

For all \gls{cv} problems, it is relevant to have a model of a camera. The most commonly used model in \gls{cv} is the pinhole camera. Figure \ref{fig:pinhole_cam} gives a simple overview of the important parts of the pinhole model.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/sfm/pinholeCamera.png}
    \caption{Overview of the components of the pinhole camera model \cite{openMVG}. The camera intrinsic parameters model the optics, the extrinsic parameters model the camera position and orientation. The image plane is the plane of the \gls{ccd} and the focal plane is the plane where the focus of the optical system is.}
    \label{fig:pinhole_cam}
\end{figure} 

The pinhole camera model can be described using the $3\times4$ camera matrix $\textbf{K}$ defined as
\begin{align}
    \textbf{K} = \begin{bmatrix}
        f\times k_u & 0           & c_u \\
        0           & f\times k_v & c_v \\
        0           & 0           & 1   \\
    \end{bmatrix} 
    \begin{bmatrix}
        \textbf{R} & t
    \end{bmatrix}, \label{eq:camera_m}
\end{align}
where $f$ is the distance between the focal plane and the image plane, $k_u$ and $k_v$ to are scaling factors, $c_u$ and $c_v$ are the coordinates of the principle point on the image plane, $R$ is a $3\times3$ rotation matrix and $t$ being a $3\times1$ translation vector. The first matrix of $\textbf{K}$ reflects the camera intrinsic parameters while the second matrix describes the extrinsic parameters.
More sophisticated pinhole camera models also include distortions. These can include one or more factors for radial and tangential distortions. A special case it the Brown T2 model which includes three radial and two tangential distortion factors.

\subsubsection{Structure-from-Motion}
\gls{sfm} uses multiple views of the same object from different camera positions to reconstruct the geometry of that object. \Gls{sfm} encompasses the recovery of the three-dimensional structure of an object as well as camera poses \cite{szeliski2010computer}. Depth information is obtained through the motion parallax created by the moving camera. Generic steps of a \gls{sfm} processing pipeline are shown in Figure \ref{fig:sfm_steps}.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.25\textwidth]{doc/thesis/0_figures/sfm/SfM.pdf}
    \caption{Generic steps of a \gls{sfm} processing pipeline.}
    \label{fig:sfm_steps}
\end{figure}

Figure \ref{fig:sfm_geometry} shows a generic observation geometry. A camera at different positions with feature points on their respective image plane and the relation between this feature point and the object point are depicted. 

\begin{figure}[htb]
    \centering
    \includegraphics[width=\textwidth]{doc/thesis/0_figures/sfm/sfm_geometry.png}
    \caption{Generic observation geometry in a \gls{sfm} problem \cite{andrews2019asteroid}. The 3D structure can be reconstructed from several point observations and intrinsic camera parameters.}
    \label{fig:sfm_geometry}
\end{figure}

To reconstruct three-dimensional points from an image series, correspondence between images needs to be found. This is achieved by detecting features in an image that can be detected and matched in multiple images. A feature can be described as a local, meaningful and detectable part of an image. Features are being used because of their high information content. Features can be image regions of sudden change, shape features or texture contours. Commonly detected features are corners, edges, junctions, blobs and lines \cite{2018comparingfeatures}. These features are described using feature descriptors which assign a distinct identity to the described feature for later matching. A range of different feature detection algorithms  exist, sometimes with a ready to use algorithm. While most feature detectors are combined with a distinct feature descriptor, it is possible to interchange these. Feature detectors are tasked with detecting feature-points in an image. Feature points are also referred to as key-points or interest-points. A common requirement for good feature descriptors and detectors is that both should be scaling, rotation and affine invariant. The most commonly known feature detectors and descriptor pairs are \gls{sift}, \gls{surf}. \gls{orb}, KAZE and \gls{akaze} \cite{2018comparingfeatures}.
% Add SIFT description?
After features are detected and described in all images, these features need to be matched, i.e. an algorithm identifies the same feature in multiple images. Depending on the descriptor, either an L1 or L2 norm for a scalar descriptor or the hamming distance for binary descriptor are used. Feature matching can be carried out between image pairs or between a longer series of images. Different geometries can be described using different mappings. The homography matrix $\textbf{H}$ describes mapping of coordinates from two different views and is defined as
\begin{align}
    x'_i = \textbf{H}x_i, \label{eq:homography_m}
\end{align}
where $x'_i$ are the coordinates of point $i$ in the second image, $x_i$ are the coordinates of point $i$ in the first image and $\textbf{H}$ is the homography matrix. However, $\textbf{H}$ describes only a purely rotating or moving camera capturing a planar scene \cite{schonberger2016structure}.
The fundamental matrix $\textbf{F}$ describes the relation between two images of the same scene for a moving camera. It relates points of uncalibrated images and is defined as
\begin{align}
    {x'_i}^{T}\textbf{F}x_i = 0, \label{eq:fundamental_m}
\end{align}
where $x'_i$ are the coordinates of point $i$ in the second image, $x_i$ are the coordinates of point $i$ in the first image and $\textbf{F}$ is the fundamental matrix. The epipolar line
\begin{align}
    l'_i = \textbf{F}x_i, \label{eq:epipolar_l}
\end{align}
where $l'_i$ is the epipolar line, $\textbf{F}$ is the fundamental matrix and $x_i$ are the coordinates of a point in the first image. This line constitutes all possible positions of point $x'_i$.

If additionally, the camera intrinsics are taken into account, the fundamental matrix becomes the essential matrix $\textbf{E}$ which is defined as
\begin{align}
    \textbf{E} = \textbf{K}'^{T}\textbf{F}\textbf{K}, \label{eq:essential_m}
\end{align}
with $\textbf{E}$ being the essential matrix, $\textbf{K}'$ being the camera matrix of the second view, $\textbf{F}$ being the fundamental matrix as defined in Eq. \ref{eq:fundamental_m} and $\textbf{K}$ being the camera matrix of the first view. Therefore, the essential matrix is intended for use in conjunction with calibrated images where the camera intrinsics are available.
If sufficient number of points are being mapped correctly using one of the transformation from Equations \ref{eq:homography_m}, \ref{eq:fundamental_m} or \ref{eq:essential_m}, the points are geometrically verified.

However after geometric verification, there might still be outliers. Therefore, outlier rejection is performed as an additional step to remove incorrect matches. Several algorithms such as \gls{ransac} \cite{fischler1981random}, \gls{acransac} \cite{moisan2012automatic}, \gls{msac} \cite{wang2009generalized} and \gls{prosac} \cite{chum2005matching} are used for outlier removal. All of these algorithms aim at robustly estimate the correct model and remove outliers. The result of this step is the view graph that relates the different views to each other with images as nodes and pairs as edges \cite{schonberger2016structure}.

Using the view graph as an input, the reconstruction process produces a three-dimensional point cloud. There are two principle methods for reconstruction, incremental and global. In case the image set is unordered, the more common approach is to use the incremental approach \cite{schonberger2016structure}.

Incremental reconstruction starts with an initial pair of views. Selecting this initial pair is a critical step as the reconstruction algorithm might not converge after using a bad initial pair. Typically, starting with a scene with many overlapping camera views constitutes a robust initialisation and results in higher accuracy because of the redundant information from many images \cite{schonberger2016structure}.
Consequently, additional images are registered to the current model based on corresponding features which can be used to triangulate additional points in already registered images. This step can includes estimating the pose of a camera and also the camera intrinsic parameters for uncalibrated images. Triangulation is an essential step of making \gls{sfm} robust by adding redundant information about existing points in a model and adding additional points to increase the coverage of the model \cite{schonberger2016structure}. To help these algorithms to converge, it is possible to provide initial pose estimates, so called priors.
Image registration and triangulation are separate process although their results have a strong link. Therefore, errors of can increase the error in the other, i.e. if the pose estimation during image registration has an error, this error propagates to the position estimate of a triangulated point. To make this process more robust Bundle Adjustment is used. It is the combined refinement of camera pose and point position estimate. A commonly used algorithm is the Levenberg-Marquardt algorithm to minimise the error \cite{schonberger2016structure, moulon2012adaptive}.

Global reconstruction uses all images in a common reference frame. While incremental reconstruction accumulates errors through adding views step-by-step, global reconstruction attempts to distribute these residuals equally for all reconstructed points. Therefore, global reconstruction pipelines try to get rid of the problem of accumulating error of incremental \gls{sfm} \cite{Moulon_2013_ICCV}.
For using the global method, it is necessary to calculate the essential matrix for all image pairs as the initial step. Commonly, rotation estimation is separated from translation and structure estimation. First a consistent set of rotations are computed in a global frame based on relative rotations of input pairs. Second, camera translations are estimated as well as the structure of the object.

Both, incremental and global \gls{sfm} methods produce a sparse point cloud as an output. Additionally, they estimate the intrinsic and extrinsic camera parameters. As a next step the sparse point cloud can be densified using \gls{mvs} techniques \cite{pagani2011dense}. Image pairs with their camera positions are being used to calculate depth maps. These depth maps can be fused to increase the number of points of the point cloud. This process mimicks human depth perception and uses this information to improve the accuracy of its point clouds.

In the next step, the point cloud is converted into a 3D model by estimating a mesh surface from the point cloud. In addition, outliers are detected and removed from the scene. Different approaches can be used which yield different results.


\subsection{Compression}
Data compression is tasked with encoding information using less bits than the original representation. Other terms for data compression are source coding or bit-rate \cite{mahdi2012implementing}. Two principle methods of compression exist, lossless and lossy compression. While it is possible to recover all information after lossless compression, lossy compression accepts a certain loss of information to achieve higher compression ratios.
Lossless compression uses statistical redundancy to decrease the number of bits necessary to encode the same information. This process is completely reversible. There are many lossless compression algorithms such as LZ, LZW, run-length coding and entropy coding.
Lossy compression is not a reversible process, i.e. information is lost that cannot be recovered. The idea of lossy compression is that different types of information are perceived differently and are therefore not equally important. Audio data can be encoded with a lossy scheme by decreasing the accuracy of acoustic components that are beyond the capabilities of most humans. Another example is, that the human eye is more sensitive to changes in luminance than it is to changes in colour. Therefore, compression can lose colour information without having a strong influence of the perceived image quality. Commonly known formats for lossy compression are MP3 or JPEG. Most lossy compression algorithms are based on transform coding, especially \gls{dct}.

\subsubsection{Image compression}
Image compression is a sub-discipline of data compression. Two common types of image compression techniques are \gls{dct}, used for example in JPEG, and \gls{dwt}, used for example in JPEG 2000. 

\Gls{dct} is a type of Fourier transform, hence it expresses finite data as a sum of cosines. In contrast to the Fourier transform itself, it employs however only real components. \Gls{dct} compresses data in form of discrete data blocks. For example, images can be compressed with differently sized image parts such as $8\times8$ pixels, commonly used in JPEG.

\Gls{dwt} uses a wavelet transform, hence it expresses data using a sum of wavelets. \Gls{dwt} is particularly well suited to compress images with high frequency components, such as a star background. It is not well-suited to compress periodic data.

\subsection{Image Processing}
Digital image processing is the use of a digital computer to process digital images through algorithms.

\subsubsection{Gaussian Filtering}
Gaussian Filtering refers to convolving an image with the two-dimensional Gaussian function. It is used to blur images and remove noise. The two-dimensional Gaussian function with a mean of zero is defined as
\begin{align}
    G(x,y) = \frac{1}{2\pi \sigma^{2}}e^{-\frac{x^2+y^2}{2\sigma^2}}, \label{eq:gauss_2d}
\end{align}
where $\sigma$ is the standard deviation of the Gaussian distribution, and $x$ and $y$ are the coordinates.
In its theoretical version, the Gaussian distribution would extend to infinity. To be practically applied in a digital system, the Gaussian distribution needs to be cut at a certain distance from the centre. The range over which the filter is applied is the kernel size which is normally related to its standard deviation, since \SI{99}{\percent} of the distribution falls within 3 standard deviations. In addition, the Gaussian distribution needs to be discretised to be usable in a digital system. The two-dimensional Gaussian filter is rotationally symmetric and larger standard deviation result in more blurring due to a wider peak.

\subsubsection{Down-sample with local means}
Down-sampling is the process of reducing the number of pixels in an image. When down-sampled with local means, it refers to taking the average of a small set of pixels and using it as the new value of a single pixel. For example, if an image shall be reduced by a factor of two, the value of four pixels of the input are averaged for a single pixel of the output. In the example below a $4\times4$ matrix is down-sampled by a scaling factor of two to give a $2\times2$ matrix.
\begin{align*}
    \begin{bmatrix}
        1  & 2  & 3  & 4\\
        5  & 6  & 7  & 8\\
        9  & 10 & 11 & 12\\
        13 & 14 & 15 & 16\\
    \end{bmatrix} 
    \rightarrow 
    \begin{bmatrix}
        3.5  & 5.5\\
        11.5 & 13.5\\ 
    \end{bmatrix}
\end{align*}

\subsection{Rendering}
Rendering is process of creating two-dimensional images from three-dimensional objects.

\subsubsection{Path Tracing}
Ray tracing is a rendering technique where the path of light rays is traced to generate pixels while simulating effects of encounters with objects. Ray tracing can simulate different effects, such as reflection, refraction, scattering and dispersion. While producing a high degree of realism in images, the computational costs are high as well. There are four types of rays: camera rays, reflection rays, transmission rays and shadow rays. Reflection and transmissions can be further categorised as either diffuse, glossy or singular. Ray tracing is a popular rendering technique where a high degree of realism is necessary since it is a realistic simulation of light transport.

Path Tracing is a special form of Ray Tracing in which not individual light rays are followed which then branch into an exponentially growing number of rays when being reflected or refracted but only a single path is followed, cutting computation time dramatically in cases with a lot of reflection, refraction and shadow rays per pixel \cite{kajiya1986rendering}.

\subsection{3D Models}
Point clouds
 -points
Mesh
 -Vertices, faces,
